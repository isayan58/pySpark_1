{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('date').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o173.csv.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Error accessing configuration file\n\tat java.base/java.util.ServiceLoader.fail(ServiceLoader.java:582)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1173)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.nextProviderClass(ServiceLoader.java:1206)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1221)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1265)\n\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1300)\n\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1385)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)\n\tat scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)\n\tat scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)\n\tat scala.collection.TraversableLike.filter(TraversableLike.scala:395)\n\tat scala.collection.TraversableLike.filter$(TraversableLike.scala:395)\n\tat scala.collection.AbstractTraversable.filter(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:725)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:207)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:537)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.nio.file.NoSuchFileException: /Users/sayanadhikary/Desktop/Development/pySpark_1/venv/lib/python3.8/site-packages/pyspark/jars/spark-sql_2.12-3.3.1.jar\n\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\n\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)\n\tat java.base/java.nio.file.Files.readAttributes(Files.java:1764)\n\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1250)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:733)\n\tat java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:849)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:838)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:248)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:177)\n\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:350)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:168)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.getOrCreate(JarFileFactory.java:91)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:132)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:175)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1165)\n\t... 34 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mcsv(\u001b[39m'\u001b[39;49m\u001b[39mappl_stock.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, inferSchema\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, header\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      2\u001b[0m df\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Desktop/Development/pySpark_1/venv/lib/python3.8/site-packages/pyspark/sql/readwriter.py:535\u001b[0m, in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Development/pySpark_1/venv/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Development/pySpark_1/venv/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Development/pySpark_1/venv/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o173.csv.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Error accessing configuration file\n\tat java.base/java.util.ServiceLoader.fail(ServiceLoader.java:582)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1173)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.nextProviderClass(ServiceLoader.java:1206)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1221)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1265)\n\tat java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1300)\n\tat java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1385)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)\n\tat scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)\n\tat scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)\n\tat scala.collection.TraversableLike.filter(TraversableLike.scala:395)\n\tat scala.collection.TraversableLike.filter$(TraversableLike.scala:395)\n\tat scala.collection.AbstractTraversable.filter(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:725)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:207)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:537)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.nio.file.NoSuchFileException: /Users/sayanadhikary/Desktop/Development/pySpark_1/venv/lib/python3.8/site-packages/pyspark/jars/spark-sql_2.12-3.3.1.jar\n\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\n\tat java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\n\tat java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)\n\tat java.base/java.nio.file.Files.readAttributes(Files.java:1764)\n\tat java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1250)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:733)\n\tat java.base/java.util.zip.ZipFile$CleanableResource$FinalizableResource.<init>(ZipFile.java:849)\n\tat java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:838)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:248)\n\tat java.base/java.util.zip.ZipFile.<init>(ZipFile.java:177)\n\tat java.base/java.util.jar.JarFile.<init>(JarFile.java:350)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103)\n\tat java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:168)\n\tat java.base/sun.net.www.protocol.jar.JarFileFactory.getOrCreate(JarFileFactory.java:91)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:132)\n\tat java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:175)\n\tat java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1165)\n\t... 34 more\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('appl_stock.csv', inferSchema=True, header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2010, 1, 4, 0, 0), Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|               Date|              Open|\n",
      "+-------------------+------------------+\n",
      "|2010-01-04 00:00:00|        213.429998|\n",
      "|2010-01-05 00:00:00|        214.599998|\n",
      "|2010-01-06 00:00:00|        214.379993|\n",
      "|2010-01-07 00:00:00|            211.75|\n",
      "|2010-01-08 00:00:00|        210.299994|\n",
      "|2010-01-11 00:00:00|212.79999700000002|\n",
      "|2010-01-12 00:00:00|209.18999499999998|\n",
      "|2010-01-13 00:00:00|        207.870005|\n",
      "|2010-01-14 00:00:00|210.11000299999998|\n",
      "|2010-01-15 00:00:00|210.92999500000002|\n",
      "|2010-01-19 00:00:00|        208.330002|\n",
      "|2010-01-20 00:00:00|        214.910006|\n",
      "|2010-01-21 00:00:00|        212.079994|\n",
      "|2010-01-22 00:00:00|206.78000600000001|\n",
      "|2010-01-25 00:00:00|202.51000200000001|\n",
      "|2010-01-26 00:00:00|205.95000100000001|\n",
      "|2010-01-27 00:00:00|        206.849995|\n",
      "|2010-01-28 00:00:00|        204.930004|\n",
      "|2010-01-29 00:00:00|        201.079996|\n",
      "|2010-02-01 00:00:00|192.36999699999998|\n",
      "+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Date', 'Open').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (weekofyear, dayofmonth, hour, dayofyear, month, year, weekofyear, \n",
    "format_number, date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|dayofyear(Date)|\n",
      "+---------------+\n",
      "|              4|\n",
      "|              5|\n",
      "|              6|\n",
      "|              7|\n",
      "|              8|\n",
      "|             11|\n",
      "|             12|\n",
      "|             13|\n",
      "|             14|\n",
      "|             15|\n",
      "|             19|\n",
      "|             20|\n",
      "|             21|\n",
      "|             22|\n",
      "|             25|\n",
      "|             26|\n",
      "|             27|\n",
      "|             28|\n",
      "|             29|\n",
      "|             32|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(dayofyear(df['Date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|hour(Date)|\n",
      "+----------+\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(hour(df['Date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+----+\n",
      "|               Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|Year|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+----+\n",
      "|2010-01-04 00:00:00|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|2010|\n",
      "|2010-01-05 00:00:00|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|2010|\n",
      "|2010-01-06 00:00:00|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|2010|\n",
      "|2010-01-07 00:00:00|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|2010|\n",
      "|2010-01-08 00:00:00|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|2010|\n",
      "|2010-01-11 00:00:00|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|2010|\n",
      "|2010-01-12 00:00:00|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|2010|\n",
      "|2010-01-13 00:00:00|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|2010|\n",
      "|2010-01-14 00:00:00|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|2010|\n",
      "|2010-01-15 00:00:00|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|2010|\n",
      "|2010-01-19 00:00:00|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|2010|\n",
      "|2010-01-20 00:00:00|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|2010|\n",
      "|2010-01-21 00:00:00|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|2010|\n",
      "|2010-01-22 00:00:00|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|2010|\n",
      "|2010-01-25 00:00:00|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|2010|\n",
      "|2010-01-26 00:00:00|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|2010|\n",
      "|2010-01-27 00:00:00|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|2010|\n",
      "|2010-01-28 00:00:00|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|2010|\n",
      "|2010-01-29 00:00:00|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|2010|\n",
      "|2010-02-01 00:00:00|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|2010|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Year\", year(df['Date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.withColumn(\"Year\", year(df['Date']))\n",
    "result = new_df.groupBy('Year').mean('Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+\n",
      "|Year|Average Closing Price|\n",
      "+----+---------------------+\n",
      "|2015|   120.03999980555547|\n",
      "|2013|    472.6348802857143|\n",
      "|2014|    295.4023416507935|\n",
      "|2012|    576.0497195640002|\n",
      "|2016|   104.60400786904763|\n",
      "|2010|    259.8424600000002|\n",
      "|2011|   364.00432532142867|\n",
      "+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = result.withColumnRenamed('avg(Close)', \"Average Closing Price\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|Year|Avg Closing|\n",
      "+----+-----------+\n",
      "|2015|     120.04|\n",
      "|2013|     472.63|\n",
      "|2014|     295.40|\n",
      "|2012|     576.05|\n",
      "|2016|     104.60|\n",
      "|2010|     259.84|\n",
      "|2011|     364.00|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select('Year', format_number('Average Closing Price', 2).alias('Avg Closing')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3eeecd4d2b984693a1941de5f54f45707b45b7f9de21cb4a4f3c2b1f00266064"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
